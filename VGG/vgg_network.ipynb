{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(\n",
    "\troot= './data/cifar'\n",
    "\t,train = True\n",
    "\t,download = True\n",
    "\t,transform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor()\n",
    "\t\t])\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_set = torchvision.datasets.CIFAR10(\n",
    "\troot= './data/cifar'\n",
    "\t,train = False\n",
    "\t,download = True\n",
    "\t,transform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor()\n",
    "\t\t])\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "targ = []\n",
    "for j in range(10):\n",
    "    val = 0\n",
    "    for i in range(len(train_set.targets)):\n",
    "        if train_set.targets[i] == j:\n",
    "            data.append(train_set.data[i])\n",
    "            targ.append(train_set.targets[i])\n",
    "            val += 1\n",
    "            if val == 400:\n",
    "                i = len(train_set.targets)\n",
    "                break\n",
    "print(len(targ))\n",
    "print(targ)\n",
    "train_set.data = data\n",
    "train_set.targets = targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * len(train_set))\n",
    "\n",
    "index_list = list(range(len(train_set)))\n",
    "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
    "\n",
    "tr_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(\n",
    "\ttrain_set, batch_size =10, sampler = tr_sampler\n",
    "\t)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "\ttrain_set, batch_size =10, sampler = val_sampler\n",
    "\t)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "\ttest_set, batch_size =10\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
    "        \"\"\"\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv5.weight)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv6.weight)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv7.weight)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv8.weight)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv9.weight)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv10.weight)\n",
    "\n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv11.weight)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv12.weight)\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding = 1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv13.weight)\n",
    "        \"\"\"\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc11 = nn.Linear(in_features = 8192, out_features = 200)\n",
    "        torch.nn.init.xavier_uniform_(self.fc11.weight)\n",
    "        self.fc21 = nn.Linear(in_features= 200, out_features = 200)\n",
    "        torch.nn.init.xavier_uniform_(self.fc21.weight)\n",
    "        self.out1 = nn.Linear(in_features= 200, out_features = 10)\n",
    "        torch.nn.init.xavier_uniform_(self.out1.weight)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        t = self.conv1(t).cuda()\n",
    "        y = nn.BatchNorm2d(64).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        #t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 1)\n",
    "        #print(\"First\")\n",
    "        #print(t)\n",
    "        t = self.conv2(t).cuda()\n",
    "        y = nn.BatchNorm2d(64).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 2).cuda()\n",
    "        #print(\"Second\")\n",
    "        #print(t)\n",
    "        \n",
    "        t = self.conv3(t).cuda()\n",
    "        y = nn.BatchNorm2d(128).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv4(t).cuda()\n",
    "        y = nn.BatchNorm2d(128).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 2).cuda()\n",
    "        \"\"\"\n",
    "        #print(\"third\")\n",
    "        #print(t)\n",
    "        t = self.conv5(t).cuda()\n",
    "        y = nn.BatchNorm2d(256).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv6(t).cuda()\n",
    "        y = nn.BatchNorm2d(256).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv7(t).cuda()\n",
    "        y = nn.BatchNorm2d(256).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 2).cuda()\n",
    "        \n",
    "        #print(\"fourth\")\n",
    "        #print(t)\n",
    "        t = self.conv8(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv9(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv10(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 2).cuda()\n",
    "        \n",
    "        #print(\"fifth\")\n",
    "        #print(t)\n",
    "        t = self.conv11(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv12(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.conv13(t).cuda()\n",
    "        y = nn.BatchNorm2d(512).cuda()\n",
    "        t = y(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = nn.functional.max_pool2d(t, kernel_size = 2, stride = 2).cuda()\n",
    "        \"\"\"\n",
    "        t = t.view(-1, 8192).cuda()\n",
    "\n",
    "        #print(\"before fc\")\n",
    "        #print(t)        \n",
    "        t = self.fc11(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.dropout(t).cuda()\n",
    "        \n",
    "       # print(\"after first fc\")\n",
    "        #print(t) \n",
    "        t= self.fc21(t).cuda()\n",
    "        t = nn.functional.relu(t).cuda()\n",
    "        t = self.dropout(t).cuda()\n",
    "        \n",
    "        t = self.out1(t).cuda()\n",
    "       # t = nn.functional.softmax(t, dim=1)\n",
    "        \n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x7fc5e94e1d68>\n",
      "Tesla K80\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = \"cuda:0\"\n",
    "network = Network()\n",
    "network.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 5e-4)\n",
    "scheduler = MultiStepLR(optimizer, milestones = [15,30,40],gamma = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc11): Linear(in_features=8192, out_features=200, bias=True)\n",
       "  (fc21): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (out1): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.load_state_dict(torch.load('./vggtrainedNew'), strict = False)\n",
    "network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.5346566331386566\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.34204881131649\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.2711468334992726\n",
      "training loss = 2.2612705286767416\n",
      "Validation loss = 4.931759146195423\n",
      "Epochs completed : 1\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1272565281391143\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.1196557235717775\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.113693733215332\n",
      "training loss = 2.113389345545754\n",
      "Validation loss = 5.496568112433711\n",
      "Epochs completed : 2\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.121334820985794\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.109458038806915\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.106330631573995\n",
      "training loss = 2.1056452903627974\n",
      "Validation loss = 5.885483633113813\n",
      "Epochs completed : 3\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1200645184516906\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.109760345220566\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.1049128925800322\n",
      "training loss = 2.1044850054205773\n",
      "Validation loss = 6.013019000427632\n",
      "Epochs completed : 4\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.118935611248016\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.103381367325783\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.0993137447039287\n",
      "training loss = 2.099103482913074\n",
      "Validation loss = 6.2121154507504235\n",
      "Epochs completed : 5\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1160500061511995\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.101807209253311\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.0983348230520886\n",
      "training loss = 2.097854912094188\n",
      "Validation loss = 6.195508787903605\n",
      "Epochs completed : 6\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.111473187208176\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.0990964329242705\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.098146544297536\n",
      "training loss = 2.098005237997886\n",
      "Validation loss = 6.2720924872386306\n",
      "Epochs completed : 7\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1105342960357665\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.1035590052604674\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.1006887634595235\n",
      "training loss = 2.1000003171938713\n",
      "Validation loss = 6.319710616823993\n",
      "Epochs completed : 8\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.115568392276764\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.104113137125969\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.1000761183102927\n",
      "training loss = 2.1001297747453553\n",
      "Validation loss = 6.367430807668952\n",
      "Epochs completed : 9\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.114270703792572\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.1029429233074186\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.098308664162954\n",
      "training loss = 2.09815698133367\n",
      "Validation loss = 6.420294707334494\n",
      "Epochs completed : 10\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1102941250801086\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.1011074817180635\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.099526439507802\n",
      "training loss = 2.098860149473232\n",
      "Validation loss = 6.397115333170831\n",
      "Epochs completed : 11\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.111119110584259\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.1022121858596803\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.0969227004051207\n",
      "training loss = 2.0969604287401635\n",
      "Validation loss = 6.450642175312284\n",
      "Epochs completed : 12\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.1113194811344145\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.0998466432094576\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.099236742655436\n",
      "training loss = 2.0977474372588727\n",
      "Validation loss = 6.502224179762829\n",
      "Epochs completed : 13\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.110147582292557\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.102066106200218\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.0911603061358135\n",
      "training loss = 2.0905276353830082\n",
      "Validation loss = 6.674569443811344\n",
      "Epochs completed : 14\n",
      "0.01\n",
      "no minibatches 100\n",
      "training loss = 2.106730686426163\n",
      "0.01\n",
      "no minibatches 200\n",
      "training loss = 2.0994329875707627\n",
      "0.01\n",
      "no minibatches 300\n",
      "training loss = 2.0958005209763844\n",
      "training loss = 2.095255417883583\n",
      "Validation loss = 6.639629949497271\n",
      "Epochs completed : 15\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 2.1091497254371645\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 2.099413241147995\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 2.093817733923594\n",
      "training loss = 2.0937902209900763\n",
      "Validation loss = 6.6481044201911255\n",
      "Epochs completed : 16\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 2.090636194944382\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 2.072321344614029\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 2.0655071051915486\n",
      "training loss = 2.0660841532261767\n",
      "Validation loss = 6.925503857528107\n",
      "Epochs completed : 17\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 2.0580479395389557\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 2.0461060279607772\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 2.034734268585841\n",
      "training loss = 2.036696376097987\n",
      "Validation loss = 6.786871662622766\n",
      "Epochs completed : 18\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 2.058156360387802\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 2.036489401459694\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 2.028400183916092\n",
      "training loss = 2.031703352554465\n",
      "Validation loss = 7.007182495503486\n",
      "Epochs completed : 19\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 2.019839916229248\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.9984499776363374\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.9842587021986644\n",
      "training loss = 1.9848867905177292\n",
      "Validation loss = 7.495350240152093\n",
      "Epochs completed : 20\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.9636000835895537\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.9403528308868407\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.9173949289321899\n",
      "training loss = 1.9182987115973589\n",
      "Validation loss = 7.566020754319203\n",
      "Epochs completed : 21\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.931198148727417\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.8899659019708634\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.8837042355537414\n",
      "training loss = 1.8865599781741917\n",
      "Validation loss = 8.071480563924283\n",
      "Epochs completed : 22\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.9023950707912445\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.8593915873765945\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.8547376175721486\n",
      "training loss = 1.8554058273010494\n",
      "Validation loss = 7.571534494810466\n",
      "Epochs completed : 23\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.825047641992569\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.814580530524254\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.8082608652114869\n",
      "training loss = 1.8056015269509678\n",
      "Validation loss = 7.609117767478846\n",
      "Epochs completed : 24\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.8320921993255614\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.788630674481392\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7903172532717386\n",
      "training loss = 1.7882354124960107\n",
      "Validation loss = 7.891594536696808\n",
      "Epochs completed : 25\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.820766054391861\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.7850441348552704\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7870930143197377\n",
      "training loss = 1.7859130171025435\n",
      "Validation loss = 7.934784472743167\n",
      "Epochs completed : 26\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.8204617738723754\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.7766055804491043\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7592308874924978\n",
      "training loss = 1.760055685865468\n",
      "Validation loss = 7.735867150222199\n",
      "Epochs completed : 27\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.775633167028427\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.7577987307310103\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7569172887007396\n",
      "training loss = 1.7639890640133227\n",
      "Validation loss = 7.735851173159443\n",
      "Epochs completed : 28\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.7462892949581146\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.7496607667207718\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7543019735813141\n",
      "training loss = 1.7528522261257828\n",
      "Validation loss = 8.070230387434174\n",
      "Epochs completed : 29\n",
      "0.002\n",
      "no minibatches 100\n",
      "training loss = 1.7717659533023835\n",
      "0.002\n",
      "no minibatches 200\n",
      "training loss = 1.7379160511493683\n",
      "0.002\n",
      "no minibatches 300\n",
      "training loss = 1.7433420077959696\n",
      "training loss = 1.7440003339773436\n",
      "Validation loss = 8.59714869607853\n",
      "Epochs completed : 30\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.7070624560117722\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.7018159064650535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6970933963855108\n",
      "training loss = 1.697875087724584\n",
      "Validation loss = 8.345723158196558\n",
      "Epochs completed : 31\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.7117730855941773\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.6987001782655715\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.670570680697759\n",
      "training loss = 1.6672638920033613\n",
      "Validation loss = 8.64623824252358\n",
      "Epochs completed : 32\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6769711363315583\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.6692999070882797\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6534441604216894\n",
      "training loss = 1.650494728903038\n",
      "Validation loss = 8.717057403129868\n",
      "Epochs completed : 33\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6950756323337555\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.654750475883484\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6611215368906658\n",
      "training loss = 1.660719927201824\n",
      "Validation loss = 8.591005162347722\n",
      "Epochs completed : 34\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6326059675216675\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.6177127861976623\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.62728560090065\n",
      "training loss = 1.6311943998157417\n",
      "Validation loss = 8.604826263234585\n",
      "Epochs completed : 35\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6522915494441985\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.624052191078663\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.627241188486417\n",
      "training loss = 1.6346377644419297\n",
      "Validation loss = 8.663973379738723\n",
      "Epochs completed : 36\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6584680545330048\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.6484272688627244\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6386201723416647\n",
      "training loss = 1.6360451939711376\n",
      "Validation loss = 8.802855594248712\n",
      "Epochs completed : 37\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6347601556777953\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.6344775587320328\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6324096683661142\n",
      "training loss = 1.6267548317445857\n",
      "Validation loss = 8.705554702613927\n",
      "Epochs completed : 38\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.6127207064628601\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.591186535358429\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6110505239168802\n",
      "training loss = 1.615917939377429\n",
      "Validation loss = 8.700778490380396\n",
      "Epochs completed : 39\n",
      "0.0004\n",
      "no minibatches 100\n",
      "training loss = 1.5949743634462357\n",
      "0.0004\n",
      "no minibatches 200\n",
      "training loss = 1.5878500261902808\n",
      "0.0004\n",
      "no minibatches 300\n",
      "training loss = 1.6137944159905115\n",
      "training loss = 1.6195020244039338\n",
      "Validation loss = 9.0406911403318\n",
      "Epochs completed : 40\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.5962361550331117\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.5980580294132232\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.6097500427563984\n",
      "training loss = 1.6125371598151037\n",
      "Validation loss = 8.835614741603031\n",
      "Epochs completed : 41\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.6158515238761901\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6239030343294143\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.614050161441167\n",
      "training loss = 1.6123045877229458\n",
      "Validation loss = 8.878718774529952\n",
      "Epochs completed : 42\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.61202576816082\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6004518869519233\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.6067466320594153\n",
      "training loss = 1.6050962893566743\n",
      "Validation loss = 8.846686260609687\n",
      "Epochs completed : 43\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.5743213725090026\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.5750713115930557\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.5802805240948994\n",
      "training loss = 1.578999355295235\n",
      "Validation loss = 8.864201334458363\n",
      "Epochs completed : 44\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.6425863909721374\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6014558574557305\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.5964791629711788\n",
      "training loss = 1.6017619556394116\n",
      "Validation loss = 9.078987399234048\n",
      "Epochs completed : 45\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.5912468999624252\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6024940434098243\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.6065630795558294\n",
      "training loss = 1.6062705174882583\n",
      "Validation loss = 8.967465219618399\n",
      "Epochs completed : 46\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.603111845254898\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6077029439806938\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.6056438034772873\n",
      "training loss = 1.6059358215257291\n",
      "Validation loss = 9.044157698184629\n",
      "Epochs completed : 47\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.5927917861938476\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.6073851817846299\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.5884575672944388\n",
      "training loss = 1.5898203375197504\n",
      "Validation loss = 8.902351717405681\n",
      "Epochs completed : 48\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.556270397901535\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.5857839411497117\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.5734176854292552\n",
      "training loss = 1.5781440734863281\n",
      "Validation loss = 9.120137281055692\n",
      "Epochs completed : 49\n",
      "8e-05\n",
      "no minibatches 100\n",
      "training loss = 1.5298274099826812\n",
      "8e-05\n",
      "no minibatches 200\n",
      "training loss = 1.5656326526403428\n",
      "8e-05\n",
      "no minibatches 300\n",
      "training loss = 1.5780011161168417\n",
      "training loss = 1.576914913228313\n",
      "Validation loss = 9.066960769363597\n",
      "Epochs completed : 50\n"
     ]
    }
   ],
   "source": [
    "loss_training = []\n",
    "loss_val = []\n",
    "for epoch in range(50):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = network(inputs)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if(i %100 == 0 and i!= 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print(param_group['lr'])\n",
    "            print(\"no minibatches \"+ str(i))\n",
    "            a = str( (running_loss)/i)\n",
    "            print(\"training loss = \" + a)\n",
    "    scheduler.step()\n",
    "    loss_training.append((running_loss)/i)\n",
    "    a = str( (running_loss)/i)\n",
    "    print(\"training loss = \" + a)\n",
    "    valloss = 0.0\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = network(inputs)\n",
    "        valloss += criterion(outputs, labels).item()\n",
    "    loss_val.append(valloss/i)\n",
    "    b = str (valloss/i)\n",
    "    print(\"Validation loss = \" + b)\n",
    "\n",
    "    print(\"Epochs completed : \"+ str(epoch +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc11): Linear(in_features=8192, out_features=200, bias=True)\n",
       "  (fc21): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (out1): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 29 %\n"
     ]
    }
   ],
   "source": [
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(\"cuda:0\"), labels.to(\"cuda:0\")\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
